{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LP_SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP8Yh2FAVGhmSYaaByybCAQ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"hjPHxsWrV5DZ"},"source":["import numpy as np\n","import pickle, copy\n","import argparse\n","import re\n","import string\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import random import shuffle\n","\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from nltk.util import ngrams\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import hstack\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.semi_supervised import LabelSpreading"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-O2iYGmX_IJ"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9zOSyG-FV-G"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--task\", required=True, help=\"USA or India\")\n","parser.add_argument(\"--data\", required=True, help=\"path of the directory containing data files\")\n","\n","args = vars(parser.parse_args())\n","data_dir = args[\"data\"]\n","task_name = args[\"task\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"haM3wn5AIozK"},"source":["with open(data_dir + '/train', 'rb') as F:\n","    train = pickle.load(F)\n","\n","with open(data_dir + '/test', 'rb') as F:\n","    test = pickle.load(F)\n","\n","with open(data_dir + '/EncodedDataFrameWithLabel', 'rb') as F:\n","    EncodedTweetDFLabel = pickle.load(F)\n","\n","TweetInfoDF = pd.read_csv(data_dir + '/TweetInfoDF.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oZbrB_qUYJA_"},"source":["def remove_punctuations_and_numbers(String):\n","    L = []\n","    for s in tokenize(String):\n","        if(s not in string.punctuation)and not(s>=\"0\" and s<=\"9\")  and not(s==\"â€¦\") and 'a'<=s and s<='z':\n","            L.append(s)\n","    return \" \".join(L)\n","\n","def tokenize(String):\n","    return nltk.word_tokenize(String)\n","\n","def remove_stopwords(List):\n","    L = []\n","    for s in List:\n","        if(s not in stopwords.words(\"english\") and (s not in hindi_stp) and (s not in hinglish_stp)):\n","            L.append(s)\n","    return L\n","\n","def strip_links(text):\n","    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n","    links         = re.findall(link_regex, text)\n","    for link in links:\n","        text = text.replace(link[0], ', ')    \n","    return text\n","\n","def strip_all_entities(text):\n","    entity_prefixes = ['@']\n","    for separator in  string.punctuation:\n","        if separator not in entity_prefixes :\n","            text = text.replace(separator,' ')\n","    words = []\n","    for word in text.split():\n","        word = word.strip()\n","        if word:\n","            if word[0] not in entity_prefixes:\n","                words.append(word)\n","    return ' '.join(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IF3L_rwYAVr"},"source":["TweetInfoDFText = list(TweetInfoDF['text'])\n","\n","EncodedTweetDFLabel['Tweet'] = list(TweetInfoDF['text'])\n","EncodedTweetDFLabel_without_label = EncodedTweetDFLabel[EncodedTweetDFLabel['Tag']==-1]\n","EncodedTweetDFLabel_without_label_INFO = copy.deepcopy(EncodedTweetDFLabel_without_label)\n","\n","EncodedTweetDFLabel_without_label_INFO_cleaned_tweets = []\n","\n","for i,j in EncodedTweetDFLabel_without_label_INFO.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    EncodedTweetDFLabel_without_label_INFO_cleaned_tweets.append(tw)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoN2n368Y340"},"source":["if task_name=='India':\n","    EncodedTweetDFLabel_without_label_INFO_cleaned_tweets = list(set(EncodedTweetDFLabel_without_label_INFO_cleaned_tweets))\n","    shuffle(EncodedTweetDFLabel_without_label_INFO_cleaned_tweets)\n","    EncodedTweetDFLabel_without_label_INFO_cleaned_tweets = EncodedTweetDFLabel_without_label_INFO_cleaned_tweets[:60000]\n","\n","EncodedTweetDFLabel_without_label_INFO_tag = [-1 for _ in EncodedTweetDFLabel_without_label_INFO_cleaned_tweets]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4kSWozgaYJvj"},"source":["cleaned_tweets = []\n","\n","for i,j in train.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","\n","train_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = []\n","\n","for i,j in test.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","\n","test_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = []\n","cleaned_tweets = test_cleaned_tweets\n","cleaned_tweets.extend(train_cleaned_tweets)\n","\n","word_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='word',\n","    stop_words='english',\n","    ngram_range=(1, 3),\n","    max_features=30000)\n","word_vectorizer.fit(cleaned_tweets)\n","dataset_word_features = word_vectorizer.transform(cleaned_tweets)\n","print(dataset_word_features.shape)\n","\n","char_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='char',\n","    stop_words='english',\n","    ngram_range=(2, 4),\n","    max_features=30000)\n","char_vectorizer.fit(cleaned_tweets)\n","dataset_char_features = char_vectorizer.transform(cleaned_tweets)\n","print(dataset_char_features.shape)\n","\n","dataset_features = hstack([dataset_char_features, dataset_word_features])\n","\n","dataset_features = dataset_features.todense()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqMi4G4CYcQV"},"source":["X_test = dataset_features[:test.shape[0]]\n","X_train = dataset_features[test.shape[0]:]\n","y_test = list(test['Tag'])\n","y_train = list(train['Tag'])\n","y_train.extend(list(EncodedTweetDFLabel_without_label_INFO_tag))\n","\n","print(X_train.shape, X_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9T9U-10QYwXf"},"source":["clf = LabelSpreading('rbf')\n","clf.fit(X_train, y_train)\n","print(clf)\n","\n","pred_y_test = clf.predict(X_test)\n","print(classification_report(y_test, pred_y_test))"],"execution_count":null,"outputs":[]}]}