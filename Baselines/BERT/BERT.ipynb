{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOfNSTUnrPzr91UpCStljQM"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Sb8Yno2ny0CO"},"source":["import torch\n","from tqdm.notebook import tqdm\n","\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset\n","\n","from transformers import BertForSequenceClassification\n","\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","\n","from sklearn.model_selection import train_test_split\n","import pickle, re, string\n","from nltk.corpus import stopwords\n","import nltk, copy\n","import numpy as np\n","import argparse\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Li6nkYEHizRe"},"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import AdamW, get_linear_schedule_with_warmup\n","from sklearn.metrics import f1_score, classification_report\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LCoecIyY0czL"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Z7QvTIZJEZn"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--data\", required=True, help=\"path of the directory containing data files\")\n","\n","args = vars(parser.parse_args())\n","data_dir = args[\"data\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LSMH4o60jaz"},"source":["with open(data_dir + '/train', 'rb') as F:\n","    train = pickle.load(F)\n","\n","with open(data_dir + '/test', 'rb') as F:\n","    test = pickle.load(F)\n","\n","TweetInfoDF = pd.read_csv(data_dir + '/TweetInfoDF.csv')\n","TweetInfoDFText = list(TweetInfoDF['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNftYjV4IzpQ"},"source":["N = len(np.unique(train['Tags']))\n","label_dict = dict()\n","\n","for i in range(N):\n","    label_dict[i] = i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jR8LKBR10jdb"},"source":["train_tw = []\n","\n","for i,j in train.iterrows():\n","    train_tw.append(TweetInfoDFText[i])\n","\n","train['Tweet'] = train_tw\n","\n","test_tw = []\n","\n","for i,j in test.iterrows():\n","    test_tw.append(TweetInfoDFText[i])\n","\n","test['Tweet'] = test_tw\n","\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","encoded_data_train = tokenizer.batch_encode_plus(\n","    train.Tweet.values, \n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    pad_to_max_length=True, \n","    max_length=256, \n","    return_tensors='pt'\n",")\n","\n","encoded_data_val = tokenizer.batch_encode_plus(\n","    test.Tweet.values, \n","    add_special_tokens=True, \n","    return_attention_mask=True, \n","    pad_to_max_length=True, \n","    max_length=256, \n","    return_tensors='pt'\n",")\n","\n","input_ids_train = encoded_data_train['input_ids']\n","attention_masks_train = encoded_data_train['attention_mask']\n","labels_train = torch.tensor(train.Tag.values)\n","\n","input_ids_val = encoded_data_val['input_ids']\n","attention_masks_val = encoded_data_val['attention_mask']\n","labels_val = torch.tensor(test.Tag.values)\n","\n","dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n","dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TO_EqIdxdwCs"},"source":["def f1_score_func(preds, labels):\n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return f1_score(labels_flat, preds_flat, average='weighted')\n","\n","def accuracy_per_class(preds, labels):\n","    label_dict_inverse = {v: k for k, v in label_dict.items()}\n","    \n","    preds_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","\n","    for label in np.unique(labels_flat):\n","        y_preds = preds_flat[labels_flat==label]\n","        y_true = labels_flat[labels_flat==label]\n","        print(f'Class: {label_dict_inverse[label]}')\n","        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')\n","    print(classification_report(labels_flat, preds_flat))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZfSA5pVgdyob"},"source":["seed_val = 17\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N2dNxT37d4n0"},"source":["def evaluate(model,dataloader_val):\n","\n","    model.eval()\n","    \n","    loss_val_total = 0\n","    predictions, true_vals = [], []\n","    \n","    for batch in dataloader_val:\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }\n","\n","        with torch.no_grad():        \n","            outputs = model(**inputs)\n","            \n","        loss = outputs[0]\n","        logits = outputs[1]\n","        loss_val_total += loss.item()\n","\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = inputs['labels'].cpu().numpy()\n","        predictions.append(logits)\n","        true_vals.append(label_ids)\n","    \n","    loss_val_avg = loss_val_total/len(dataloader_val) \n","    \n","    predictions = np.concatenate(predictions, axis=0)\n","    true_vals = np.concatenate(true_vals, axis=0)\n","            \n","    return loss_val_avg, predictions, true_vals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223,"referenced_widgets":["e8fa804534084976b9de7780e5ab4e45","beb80243d2fc4d9e88b6449ad6c2cc33","84d5ebf443ce42ecb74a89346c6bc600","7632429d6d3545ad81782ffdda30c84a","7d42800c28e0475c8ee60c6bf9e7f775","70d6057673eb41618a40636de5a1e1e4","d069bcb85fce4dc2bb903ab58c6a090d","eff9f2399ebb4ddeac38ef7ab10c375d","8ac64a45ce404c148a8a4706eb3539cf","c0f80610fd424c03a9b4235656aec582","7f2a5f8107dd4ce6901bf4f953086af9","9d6262ecbe23480cb831f9c5e1b1c884","b9fd76585fd04acc8398368a9fe41ecc","ef43b123032542f1a9a3e3440a66b079","92cd9af335054913b19381f197fc3c78","7ad1574e830849428707e1491c5d0749"]},"id":"vunUy_wmXJH6","executionInfo":{"status":"ok","timestamp":1611428575402,"user_tz":-330,"elapsed":20849,"user":{"displayName":"Samiya Caur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2n0t4GHaw9xARSIfKBQgiUHxme9EkMwnzdS9usA=s64","userId":"02348889184279128207"}},"outputId":"2d4c9c83-6ee7-4637-b76e-4d0f906d7d9d"},"source":["model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n","                                                      num_labels=len(label_dict),\n","                                                      output_attentions=False,\n","                                                      output_hidden_states=False)\n","\n","model.to(device)\n","\n","batch_size = 3\n","\n","dataloader_train = DataLoader(dataset_train, \n","                              sampler=RandomSampler(dataset_train), \n","                              batch_size=batch_size)\n","\n","dataloader_validation = DataLoader(dataset_val, \n","                                   sampler=SequentialSampler(dataset_val), \n","                                   batch_size=batch_size)\n","\n","optimizer = AdamW(model_500.parameters(), lr=1e-5, eps=1e-8)\n","\n","epochs = 10\n","\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps=0,\n","                                            num_training_steps=len(dataloader_train)*epochs)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8fa804534084976b9de7780e5ab4e45","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8ac64a45ce404c148a8a4706eb3539cf","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"be1sqqgWdomp"},"source":["for epoch in tqdm(range(1, epochs+1)):\n","    \n","    model.train()\n","    \n","    loss_train_total = 0\n","\n","    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n","    for batch in progress_bar:\n","\n","        model.zero_grad()\n","        \n","        batch = tuple(b.to(device) for b in batch)\n","        \n","        inputs = {'input_ids':      batch[0],\n","                  'attention_mask': batch[1],\n","                  'labels':         batch[2],\n","                 }       \n","\n","        outputs = model(**inputs)\n","        \n","        loss = outputs[0]\n","        loss_train_total += loss.item()\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n","         \n","    tqdm.write(f'\\nEpoch {epoch}')\n","    \n","    loss_train_avg = loss_train_total/len(dataloader_train)            \n","    tqdm.write(f'Training loss: {loss_train_avg}')\n","    \n","    val_loss, predictions, true_vals = evaluate(model, dataloader_validation)\n","    val_f1 = f1_score_func(predictions, true_vals)\n","    tqdm.write(f'Validation loss: {val_loss}')\n","    tqdm.write(f'F1 Score (Weighted): {val_f1}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbS-OKy0sJk3"},"source":["_, predictions, true_vals = evaluate(model, dataloader_validation)\n","accuracy_per_class(predictions, true_vals)"],"execution_count":null,"outputs":[]}]}