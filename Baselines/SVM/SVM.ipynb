{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPcaJy/Kh+O8ihIBS0GhgV+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"cBZIZlpNww6C"},"source":["import numpy as np\n","import pickle, copy\n","\n","import re, string\n","from nltk.corpus import stopwords\n","import pandas as pd\n","import argparse\n","from sklearn import svm\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from nltk.util import ngrams\n","import nltk\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import hstack\n","from sklearn.utils.class_weight import compute_class_weight"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8I5RI3NaxGQ0"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iN3OAA_rLeQe"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--data\", required=True, help=\"path of the directory containing data files\")\n","\n","args = vars(parser.parse_args())\n","data_dir = args[\"data\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Olt7c6HXxTmV"},"source":["with open(data_dir + '/train', 'rb') as F:\n","    train = pickle.load(F)\n","\n","with open(data_dir + '/test', 'rb') as F:\n","    test = pickle.load(F)\n","\n","TweetInfoDF = pd.read_csv(data_dir + '/TweetInfoDF.csv')\n","TweetInfoDFText = list(TweetInfoDF['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"inj_YuwixKSG"},"source":["def remove_punctuations_and_numbers(String):\n","    L = []\n","    for s in tokenize(String):\n","        if(s not in string.punctuation)and not(s>=\"0\" and s<=\"9\")  and not(s==\"â€¦\") and 'a'<=s and s<='z':\n","            L.append(s)\n","    return \" \".join(L)\n","\n","def tokenize(String):\n","    return nltk.word_tokenize(String)\n","\n","def remove_stopwords(List):\n","    L = []\n","    for s in List:\n","        if(s not in stopwords.words(\"english\") and (s not in hindi_stp) and (s not in hinglish_stp)):\n","            L.append(s)\n","    return L\n","\n","def strip_links(text):\n","    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n","    links         = re.findall(link_regex, text)\n","    for link in links:\n","        text = text.replace(link[0], ', ')    \n","    return text\n","\n","def strip_all_entities(text):\n","    entity_prefixes = ['@']\n","    for separator in  string.punctuation:\n","        if separator not in entity_prefixes :\n","            text = text.replace(separator,' ')\n","    words = []\n","    for word in text.split():\n","        word = word.strip()\n","        if word:\n","            if word[0] not in entity_prefixes:\n","                words.append(word)\n","    return ' '.join(words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"egzP1po2xUL_"},"source":["cleaned_tweets = []\n","\n","for i,j in train.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","\n","train_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = []\n","\n","for i,j in test.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","\n","test_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = []\n","cleaned_tweets = test_cleaned_tweets\n","cleaned_tweets.extend(train_cleaned_tweets)\n","\n","word_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='word',\n","    stop_words='english',\n","    ngram_range=(1, 3),\n","    max_features=30000)\n","word_vectorizer.fit(cleaned_tweets)\n","dataset_word_features = word_vectorizer.transform(cleaned_tweets)\n","print(dataset_word_features.shape)\n","\n","char_vectorizer = TfidfVectorizer(\n","    sublinear_tf=True,\n","    strip_accents='unicode',\n","    analyzer='char',\n","    stop_words='english',\n","    ngram_range=(2, 4),\n","    max_features=30000)\n","char_vectorizer.fit(cleaned_tweets)\n","dataset_char_features = char_vectorizer.transform(cleaned_tweets)\n","print(dataset_char_features.shape)\n","\n","dataset_features = hstack([dataset_char_features, dataset_word_features])\n","\n","dataset_features = dataset_features.todense()\n","X_train=[]\n","X_test=[]\n","y_train=[]\n","y_test=[]\n","\n","index_ = 0\n","for i,j in test.iterrows():\n","    X_test.append(dataset_features[index_])\n","    y_test.append(j['Tag'])\n","    index_ = index_+1\n","\n","for i,j in train.iterrows():\n","    X_train.append(dataset_features[index_])\n","    y_train.append(j['Tag'])\n","    index_ = index_+1\n","\n","\n","X_train = np.asarray(X_train)\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[-1])\n","X_test = np.asarray(X_test)\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[-1])  \n","y_train = y_train\n","y_test = y_test\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wyLPbci9YaMr","executionInfo":{"status":"ok","timestamp":1611463471267,"user_tz":-330,"elapsed":178218,"user":{"displayName":"Samiya Caur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2n0t4GHaw9xARSIfKBQgiUHxme9EkMwnzdS9usA=s64","userId":"02348889184279128207"}},"outputId":"599c6875-1c60-4f1b-9d85-a5e80bbe1d45"},"source":["clf = svm.SVC(kernel='linear', class_weight='balanced')\n","clf.fit(X_train, y_train)\n","print(clf)\n","pred_y_test = clf.predict(X_test)\n","print(classification_report(y_test, pred_y_test))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SVC(C=1.0, break_ties=False, cache_size=200, class_weight='balanced', coef0=0.0,\n","    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n","    max_iter=-1, probability=False, random_state=None, shrinking=True,\n","    tol=0.001, verbose=False)\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.99      0.87      2285\n","           1       1.00      0.07      0.13        73\n","           2       0.89      0.55      0.68       917\n","           3       0.00      0.00      0.00        62\n","           4       1.00      0.09      0.17       169\n","\n","    accuracy                           0.79      3506\n","   macro avg       0.73      0.34      0.37      3506\n","weighted avg       0.81      0.79      0.75      3506\n","\n"],"name":"stdout"}]}]}