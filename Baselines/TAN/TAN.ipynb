{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TAN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOEvqowgmXGmg50C36sC3lU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"jAKmZucM2oL6"},"source":["from time import time\n","import pandas as pd\n","import json\n","from tqdm import tqdm\n","import numpy as np\n","from gensim.models import KeyedVectors\n","import os, copy, pickle, string, re, nltk\n","from scipy.stats import truncnorm\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from itertools import product\n","from tensorflow.python.keras.utils import losses_utils\n","from sklearn.utils import class_weight\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\n","from gensim.models import word2vec, Phrases\n","from sklearn.metrics import classification_report\n","import argparse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NqlxN6ZtAOGw"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cu8v_v23K3V7"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--data\", required=True, help=\"path of the directory containing data files\")\n","parser.add_argument(\"--target\", required=True)\n","\n","args = vars(parser.parse_args())\n","data_dir = args[\"data\"]\n","target_ = args[\"target\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BlRTkPoEAw_e"},"source":["with open(data_dir + '/train', 'rb') as F:\n","    train = pickle.load(F)\n","\n","with open(data_dir + '/test', 'rb') as F:\n","    test = pickle.load(F)\n","\n","TweetInfoDF = pd.read_csv(data_dir + '/TweetInfoDF.csv')\n","TweetInfoDFText = list(TweetInfoDF['text'])\n","num_classes = len(np.unique(train['Tags']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGUCsR8BAxED"},"source":["def remove_punctuations_and_numbers(String):\n","    L = []\n","    for s in tokenize(String):\n","        if(s not in string.punctuation)and not(s>=\"0\" and s<=\"9\")  and not(s==\"â€¦\") and 'a'<=s and s<='z':\n","            L.append(s)\n","    return \" \".join(L)\n","\n","def tokenize(String):\n","    return nltk.word_tokenize(String)\n","\n","def remove_stopwords(List):\n","    L = []\n","    for s in List:\n","        if(s not in stopwords.words(\"english\") and (s not in hindi_stp) and (s not in hinglish_stp)):\n","            L.append(s)\n","    return L\n","\n","def strip_links(text):\n","    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n","    links         = re.findall(link_regex, text)\n","    for link in links:\n","        text = text.replace(link[0], ', ')    \n","    return text\n","\n","def strip_all_entities(text):\n","    entity_prefixes = ['@']\n","    for separator in  string.punctuation:\n","        if separator not in entity_prefixes :\n","            text = text.replace(separator,' ')\n","    words = []\n","    for word in text.split():\n","        word = word.strip()\n","        if word:\n","            if word[0] not in entity_prefixes:\n","                words.append(word)\n","    return ' '.join(words)\n","\n","\n","def encoding(max_length, TAG_TWEET, TAG):\n","    text_tokenizer = Tokenizer()\n","\n","    text_tokenizer.fit_on_texts(TAG_TWEET)\n","    INDEXES = text_tokenizer.word_index\n","    \n","    ENCODED_TAG_TWEET = []\n","    \n","    for xyz in TAG_TWEET:\n","        tok = tokenize(xyz)\n","        LI = []\n","        for _ in tok:\n","            LI.append(INDEXES[_])\n","        ENCODED_TAG_TWEET.append(LI)\n","    print(len(ENCODED_TAG_TWEET))\n","\n","    ENCODED_TAG_TWEET=pad_sequences(ENCODED_TAG_TWEET,maxlen=max_length,padding='post',value=0.0)\n","\n","    return (text_tokenizer,pd.DataFrame(list(zip(TAG, ENCODED_TAG_TWEET)), columns =['Tag', 'Tweet']))\n","\n","def generate_sample_weights(training_data, class_weight_dictionary): \n","    sample_weights = [class_weight_dictionary[np.where(one_hot_row==1)[0][0]] for one_hot_row in training_data]\n","    return np.asarray(sample_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bVHWLe8kZBgH"},"source":["cleaned_tweets = []\n","L = 0\n","\n","VOCAB = []\n","\n","for i,j in train.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","    L_ = len(tokenize(tw))\n","    VOCAB.extend(tokenize(tw))\n","    if L_>L:\n","        L=L_\n","\n","train_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = []\n","\n","for i,j in test.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","    L_ = len(tokenize(tw))\n","    if L_>L:\n","        L=L_\n","\n","test_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = copy.deepcopy(train_cleaned_tweets)\n","cleaned_tweets.extend(test_cleaned_tweets)\n","TagList = pd.concat([train['Tag'], test['Tag']])\n","text_tokenizer, encodedtexttag = encoding(L, cleaned_tweets, TagList)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y-V1bYA1dpab"},"source":["X_train = copy.deepcopy(train)\n","X_test = copy.deepcopy(test)  \n","y_train = copy.deepcopy(train['Tag'])\n","y_test = copy.deepcopy(test['Tag'])\n","class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n","\n","y_train = mapping(y_train)\n","y_train = np.asarray(y_train)\n","y_train = y_train.reshape(y_train.shape[0], y_train.shape[2])\n","print(y_train.shape)\n","\n","y_test = mapping(y_test)\n","y_test = np.asarray(y_test)\n","y_test = y_test.reshape(y_test.shape[0], y_test.shape[2])\n","\n","sample_weights_train = generate_sample_weights(y_train, class_weights)\n","\n","INDEXES = text_tokenizer.word_index\n","tok = tokenize(target_)\n","LI = []\n","for _ in tok:\n","    LI.append(INDEXES[_])\n","target_seq = pad_sequences([LI],maxlen=L,padding='post',value=0.0)\n","target_seq = target_seq.reshape(target_seq.shape[1],)\n","\n","X_train['cleaned_tweet'] = train_cleaned_tweets\n","X_test['cleaned_tweet'] = test_cleaned_tweets\n","X_train['encoded_cleaned_tweet'] = list(encodedtexttag['Tweet'][:len(train_cleaned_tweets)])\n","X_test['encoded_cleaned_tweet'] = list(encodedtexttag['Tweet'][len(train_cleaned_tweets):])\n","X_train['target'] = [target_seq]*len(train_cleaned_tweets)\n","X_test['target'] = [target_seq]*len(test_cleaned_tweets)\n","\n","del X_train['Text']\n","del X_train['Hashtag']\n","del X_test['Text']\n","del X_test['Hashtag']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGi9heFdM4-A"},"source":["class BLSTM_encoder (tf.keras.layers.Layer):\n","  def __init__(self, hidden_dim, **kwargs):\n","    super(BLSTM_encoder, self).__init__()\n","    self.encoder = tf.keras.layers.Bidirectional(\n","        tf.keras.layers.LSTM(hidden_dim, return_sequences=True)\n","    )\n","  def call(self, x):\n","    return self.encoder(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-wWRTvzhTtew"},"source":["class Target_attention (tf.keras.layers.Layer):\n","  def __init__(self):\n","    super(Target_attention, self).__init__()\n","    self.attention_ff = tf.keras.layers.Dense(1)\n","    self.concat = tf.keras.layers.Concatenate(axis=-1)\n","  def call(self, word_input, target_input):\n","    target_input = tf.expand_dims(target_input, 1)\n","    target_input = tf.repeat(target_input, repeats=word_input.shape[1], axis=1)\n","    aug_input = self.concat([word_input, target_input])\n","    att_value = self.attention_ff(aug_input)\n","    att_value = tf.squeeze(att_value, [-1])\n","    return tf.expand_dims(tf.nn.softmax(att_value), axis=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LcwF3xUprQED"},"source":["class Stance_output (tf.keras.layers.Layer):\n","  def __init__(self, num_classes=7, **kwargs):\n","    super(Stance_output, self).__init__()\n","    self.out_ff = tf.keras.layers.Dense(num_classes, activation='softmax')\n","\n","  def call(self, lstm_out, att_score):\n","    pre_ff = tf.reduce_mean(lstm_out * att_score, axis=-2)\n","    return self.out_ff(pre_ff)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a46XD75T0pek"},"source":["class TAN_model (tf.keras.models.Model):\n","  def __init__(self,\n","               embedding_dim=128,\n","               vocab_size=10000,\n","               hidden_dim=64,\n","               num_classes=7,\n","               drop_rate=0.2,\n","               **kwargs):\n","    super(TAN_model, self).__init__()\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.encoder = BLSTM_encoder(hidden_dim)\n","    self.attention = Target_attention()\n","    self.stance_output = Stance_output(num_classes=num_classes)\n","    self.dropout1 = tf.keras.layers.Dropout(drop_rate)\n","    self.dropout2 = tf.keras.layers.Dropout(drop_rate)\n","\n","  def call(self, tweet, target, training):\n","    tweet_emb = self.embedding(tweet)\n","    target_emb = self.embedding(target)\n","    target_emb = tf.reshape(target_emb,\n","                            [target_emb.shape[0], target_emb.shape[1]*target_emb.shape[2]])\n","    tweet_enc = self.encoder(tweet_emb)\n","    tweet_enc = self.dropout1(tweet_enc)\n","    att_score = self.attention(tweet_emb, target_emb)\n","    att_score = self.dropout2(att_score)\n","    outputs = self.stance_output(tweet_enc, att_score)\n","    return outputs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7xaWiXq2zCg"},"source":["def train_step(tweet_list, actual_v, target, sample_weights_list):\n","    tweet_list = tf.convert_to_tensor(tweet_list, dtype=tf.int32)\n","    target = tf.convert_to_tensor(target, dtype=tf.int32)\n","\n","    with tf.GradientTape(persistent=True) as tape:\n","        prediction = tan_model(tf.convert_to_tensor(tweet_list), target)\n","        loss = cce(prediction, actual_v, sample_weight=sample_weights_list)\n","        epoch_accuracy.update_state(actual_v, prediction)\n","        \n","    grads = tape.gradient(loss, tan_model.trainable_variables)\n","    grads = [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(\n","        tan_model.trainable_variables, grads)]\n","    optimizer.apply_gradients(zip(grads, tan_model.trainable_variables))\n","\n","    train_loss.update_state(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CssRekcDXHua"},"source":["tan_model = TAN_model(vocab_size=len(VOCAB), num_classes=num_classes, embedding_dim=256)\n","learning_rate = 1e-2\n","optimizer=keras.optimizers.Adam(learning_rate)\n","cce = tf.keras.losses.CategoricalCrossentropy(reduction=tf.keras.losses.Reduction.AUTO)\n","epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n","train_loss = tf.keras.metrics.Mean(name='train_loss')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ULBbenz-gmzb"},"source":["batches = []\n","batch_size = 32\n","\n","st = 0\n","\n","while(st<X_train.shape[0]):\n","    if st+batch_size <X_train.shape[0]:\n","        batches.append([X_train[st:st+batch_size]['encoded_cleaned_tweet'],\n","                        y_train[st:st+batch_size],\n","                        X_train[st:st+batch_size]['target'],\n","                        sample_weights_train[st:st+batch_size]])\n","    else:\n","        batches.append([X_train[st:]['encoded_cleaned_tweet'],y_train[st:],\n","                        X_train[st:]['target'],\n","                        sample_weights_train[st:]])\n","\n","    st = st+batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKyF-zTcNRDX"},"source":["n_epoch = 10\n","\n","pbar = tf.keras.utils.Progbar(target=n_epoch*len(batches), width=15, interval=0.005,\n","                              stateful_metrics=['train_loss', 'accuracy'])\n","for epoch in range(0,n_epoch):\n","    \n","    for encoded_tweet_list, tag, target, sample_weights_list in batches:\n","          tag_ = np.array(tag)\n","          train_step(list(encoded_tweet_list), tag_, list(target), sample_weights_list)\n","          pbar.add(1, values=[(\"train_loss\", train_loss.result()),\n","                              (\"accuracy\", epoch_accuracy.result())])\n","    train_loss.reset_states()\n","    epoch_accuracy.reset_states()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xsLDcLukTFa"},"source":["Tar = [target_seq for _ in range(len(X_test))]\n","Tar = tf.convert_to_tensor(Tar)\n","\n","val_batches = []\n","\n","st = 0\n","while(st<X_test.shape[0]):\n","    if st+batch_size <X_test.shape[0]:\n","        val_batches.append([X_test[st:st+batch_size]['encoded_cleaned_tweet'],\n","                            X_test[st:st+batch_size]['target']])\n","    else:\n","        val_batches.append([X_test[st:]['encoded_cleaned_tweet'],\n","                            X_test[st:]['target']])\n","\n","    st = st+batch_size\n","\n","pred_y = []\n","\n","for encoded_tweet_list, tag in val_batches:\n","    p_y = tan_model(tf.convert_to_tensor(list(encoded_tweet_list)),\n","                    tf.convert_to_tensor(list(tag)))\n","    pred_y.extend(p_y)\n","\n","print(classification_report(np.argmax(y_test, axis=1), np.argmax(pred_y, axis=1)))"],"execution_count":null,"outputs":[]}]}