{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SiamNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOS0pFsDR/onpOelU5nImk8"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"j8JAk2ErsJA5"},"source":["from time import time\n","import pandas as pd\n","import json\n","from tqdm import tqdm\n","import numpy as np\n","from gensim.models import KeyedVectors\n","import os, copy, pickle, string, re, nltk\n","from scipy.stats import truncnorm\n","from collections import Counter\n","from sklearn.model_selection import train_test_split\n","from itertools import product\n","from tensorflow.python.keras.utils import losses_utils\n","from sklearn.utils import class_weight\n","import tensorflow as tf\n","import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.text import one_hot\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Layer\n","from tensorflow.keras.models import Model, Sequential\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Dropout\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A9cOOOjRsMfA"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YR3wGG7wvndG"},"source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--data\", required=True, help=\"path of the directory containing data files\")\n","parser.add_argument(\"--target\", required=True)\n","parser.add_argument(\"--glove_vector_file\", required=True)\n","\n","args = vars(parser.parse_args())\n","data_dir = args[\"data\"]\n","TARGET = args[\"target\"]\n","GLOVE_FILE = args[\"glove_vector_file\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZB7zZdaMsyS-"},"source":["with open(data_dir + '/train', 'rb') as F:\n","    train = pickle.load(F)\n","\n","with open(data_dir + '/test', 'rb') as F:\n","    test = pickle.load(F)\n","\n","TweetInfoDF = pd.read_csv(data_dir + '/TweetInfoDF.csv')\n","TweetInfoDFText = list(TweetInfoDF['text'])\n","num_class = len(np.unique(train['Tags']))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vRkmgKN3sVEG"},"source":["def load_glove_matrix(vec_file):\n","    word2vec = {}\n","    with open(vec_file, encoding='utf8') as f:\n","        for line in f:\n","            values = line.split()\n","            word = values[0]\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            word2vec[word] = coefs\n","    print('Found %s word vectors.' % len(word2vec))\n","    return word2vec\n","\n","word2vec = load_glove_matrix(GLOVE_FILE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"klr7bUrZswPs"},"source":["def remove_punctuations_and_numbers(String):\n","    L = []\n","    for s in tokenize(String):\n","        if(s not in string.punctuation)and not(s>=\"0\" and s<=\"9\")  and not(s==\"â€¦\") and 'a'<=s and s<='z':\n","            L.append(s)\n","    return \" \".join(L)\n","\n","def tokenize(String):\n","    return nltk.word_tokenize(String)\n","\n","def remove_stopwords(List):\n","    L = []\n","    for s in List:\n","        if(s not in stopwords.words(\"english\") and (s not in hindi_stp) and (s not in hinglish_stp)):\n","            L.append(s)\n","    return L\n","\n","def strip_links(text):\n","    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n","    links         = re.findall(link_regex, text)\n","    for link in links:\n","        text = text.replace(link[0], ', ')    \n","    return text\n","\n","def strip_all_entities(text):\n","    entity_prefixes = ['@']\n","    for separator in  string.punctuation:\n","        if separator not in entity_prefixes :\n","            text = text.replace(separator,' ')\n","    words = []\n","    for word in text.split():\n","        word = word.strip()\n","        if word:\n","            if word[0] not in entity_prefixes:\n","                words.append(word)\n","    return ' '.join(words)\n","\n","def generate_sample_weights(training_data, class_weight_dictionary): \n","    sample_weights = [class_weight_dictionary[np.where(one_hot_row==1)[0][0]] for one_hot_row in training_data]\n","    return np.asarray(sample_weights)\n","\n","def get_truncated_normal(mean=0, sd=1, low=0, upp=10):\n","    return truncnorm(\n","        (low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n","\n","def save_glove_matrix(word2vec, word_index, output_file, mean,max_val,std_dev):\n","    MAX_NB_WORDS = 200000\n","    WORD_EMBEDDING_DIM = 200\n","\n","    nb_words = min(MAX_NB_WORDS, len(word_index)) + 1\n","    embedding_matrix = np.zeros((nb_words, WORD_EMBEDDING_DIM))\n","    for word, i in tqdm(word_index.items()):\n","        embedding_vector = word2vec.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","        else:\n","            embedding_matrix[i] = get_truncated_normal(mean=mean, sd=std_dev, upp=max_val).rvs(WORD_EMBEDDING_DIM)\n","\n","    print('Vocabulary size: %d' % len(word_index))\n","    print('Valid word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) != 0))\n","    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n","\n","    print('saving glove matrix: %s ...' % output_file)\n","    np.save(output_file, embedding_matrix)\n","    print('saved.')\n","\n","def encoding(max_length, TAG_TWEET, TAG):\n","    text_tokenizer = Tokenizer()\n","\n","    text_tokenizer.fit_on_texts(TAG_TWEET)\n","    INDEXES = text_tokenizer.word_index\n","    \n","    ENCODED_TAG_TWEET = []\n","    \n","    for xyz in TAG_TWEET:\n","        tok = tokenize(xyz)\n","        LI = []\n","        for _ in tok:\n","            LI.append(INDEXES[_])\n","        ENCODED_TAG_TWEET.append(LI)\n","    print(len(ENCODED_TAG_TWEET))\n","\n","    ENCODED_TAG_TWEET=pad_sequences(ENCODED_TAG_TWEET,maxlen=max_length,padding='post',value=0.0)\n","\n","    return (text_tokenizer,pd.DataFrame(list(zip(TAG, ENCODED_TAG_TWEET)), columns =['Tag', 'Tweet']))\n","\n","def mapping(TagList):\n","    NewTagList = []\n","    for tag in TagList:\n","        L = np.zeros((1,num_class))\n","        tag_list = [tag]\n","        for m in tag_list:\n","            L[0,m] = 1\n","        NewTagList.append(L)\n","    return NewTagList"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zLssaXh_tFUL"},"source":["cleaned_tweets = []\n","L = 0\n","\n","for i,j in train.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","    L_ = len(tokenize(tw))\n","    if L_>L:\n","        L=L_\n","\n","train_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = []\n","\n","for i,j in test.iterrows():\n","    tw = TweetInfoDFText[i].lower()\n","    tw = strip_links(strip_all_entities(tw))\n","    tw = remove_punctuations_and_numbers(tw)\n","    cleaned_tweets.append(tw)\n","    L_ = len(tokenize(tw))\n","    if L_>L:\n","        L=L_\n","\n","test_cleaned_tweets = copy.deepcopy(cleaned_tweets)\n","\n","cleaned_tweets = copy.deepcopy(train_cleaned_tweets)\n","cleaned_tweets.extend(test_cleaned_tweets)\n","TagList = pd.concat([train['Tag'], test['Tag']])\n","text_tokenizer, encodedtexttag = encoding(L, cleaned_tweets, TagList)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d2Ah5YIMtGmD"},"source":["X_train = copy.deepcopy(train)\n","X_test = copy.deepcopy(test)  \n","y_train = copy.deepcopy(train['Tag'])\n","y_test = copy.deepcopy(test['Tag'])\n","class_weights = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\n","\n","y_train = mapping(y_train)\n","y_train = np.asarray(y_train)\n","y_train = y_train.reshape(y_train.shape[0], y_train.shape[2])\n","print(y_train.shape)\n","\n","y_test = mapping(y_test)\n","y_test = np.asarray(y_test)\n","y_test = y_test.reshape(y_test.shape[0], y_test.shape[2])\n","\n","sample_weights_train = generate_sample_weights(y_train, class_weights)\n","\n","word_index = text_tokenizer.word_index\n","\n","tok = tokenize(TARGET)\n","LI = []\n","for _ in tok:\n","    LI.append(word_index[_])\n","target_seq = pad_sequences([LI],maxlen=L,padding='post',value=0.0)\n","target_seq = target_seq.reshape(target_seq.shape[1],)\n","\n","X_train['cleaned_tweet'] = train_cleaned_tweets\n","X_test['cleaned_tweet'] = test_cleaned_tweets\n","X_train['encoded_cleaned_tweet'] = list(encodedtexttag['Tweet'][:len(train_cleaned_tweets)])\n","X_test['encoded_cleaned_tweet'] = list(encodedtexttag['Tweet'][len(train_cleaned_tweets):])\n","X_train['target'] = [target_seq]*len(train_cleaned_tweets)\n","X_test['target'] = [target_seq]*len(test_cleaned_tweets)\n","\n","del X_train['Text']\n","del X_train['Hashtag']\n","del X_test['Text']\n","del X_test['Hashtag']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zNSSRFs3tJ6n"},"source":["W2VEC = []\n","\n","for s in word2vec:\n","    if len(word2vec[s])==200:\n","        W2VEC.append(word2vec[s])\n","\n","W2VEC = np.asarray(W2VEC)\n","W2VEC.shape\n","\n","mean = np.mean(W2VEC)\n","max_val = np.max(W2VEC)\n","std_dev = np.std(W2VEC)\n","\n","print(mean,max_val,std_dev)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ROgIZ5c0tQBO"},"source":["save_glove_matrix(word2vec, word_index,\n","                  data_dir+\"glove_matrix\",\n","                  mean, max_val, std_dev)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VSmKCpy4tSid"},"source":["embedding_matrix_1500 = np.load(open(data_dir+\"glove_matrix.npy\", 'rb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yMHkNH2sgbgS"},"source":["num_tokens = len(word_index) + 1\n","batch_size = 32\n","n_epoch = 10\n","hidden_dim = 265*2\n","embedding_dim = 200"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oz63i_K0pyiI"},"source":["class InverseExpManDist(Layer):\n","    def __init__(self, **kwargs):\n","        self.result = None\n","        super(InverseExpManDist, self).__init__(**kwargs)\n","\n","    # input_shape will automatic collect input shapes to build layer\n","    def build(self, input_shape):\n","        super(InverseExpManDist, self).build(input_shape)\n","\n","    # This is where the layer's logic lives.\n","    def call(self, x, **kwargs):\n","        self.result = K.exp(-K.sum(K.abs(x[0] - x[1]), axis=1, keepdims=True))\n","        return self.result\n","\n","    # return output shape\n","    def compute_output_shape(self, input_shape):\n","        return K.int_shape(self.result)\n","\n","class AttentionLayer(Layer):\n","    \"\"\"\n","    Hierarchial Attention Layer as described by Hierarchical Attention Networks for Document Classification(2016)\n","    - Yang et. al.\n","    Source: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n","    Theano backend\n","    \"\"\"\n","    def __init__(self,attention_dim=100,return_coefficients=False,**kwargs):\n","        # Initializer \n","        self.return_coefficients = return_coefficients\n","        self.init = tf.keras.initializers.get('glorot_uniform') # initializes values with uniform distribution\n","        self.attention_dim = attention_dim\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)),name='W')\n","        self.b = K.variable(self.init((self.attention_dim, )),name='b')\n","        self.u = K.variable(self.init((self.attention_dim, 1)),name='u')\n","        self.trainable_weights = [self.W, self.b, self.u]\n","\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, hit):\n","        # Here, the actual calculation is done\n","        uit = K.bias_add(K.dot(hit, self.W),self.b)\n","        uit = K.tanh(uit)\n","        \n","        ait = K.dot(uit, self.u)\n","        ait = K.squeeze(ait, -1)\n","        ait = K.exp(ait)\n","        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        ait = K.expand_dims(ait)\n","        weighted_input = hit * ait\n","        \n","        if self.return_coefficients:\n","            return [K.sum(weighted_input, axis=1), ait]\n","        else:\n","            return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        if self.return_coefficients:\n","            return [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[-1], 1)]\n","        else:\n","            return input_shape[0], input_shape[-1]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G46LFTwANQ81"},"source":["class Model (tf.keras.models.Model):\n","  def __init__(self,\n","               embedding_dim=embedding_dim,\n","               vocab_size=num_tokens,\n","               hidden_dim=hidden_dim,\n","               num_classes=5,\n","               drop_rate=0.2,\n","               **kwargs):\n","    super(Model, self).__init__()\n","    self.embedding = embedding_layer = Embedding(num_tokens, embedding_dim,\n","                                                 embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","                                                 trainable=False,)\n","    self.bilstm = Bidirectional(tf.keras.layers.LSTM(hidden_dim,\n","                                                     recurrent_dropout=drop_rate,\n","                                                     return_sequences=True))\n","    self.attention = AttentionLayer(embedding_dim)\n","    self.malstm_distance = InverseExpManDist()\n","    self.dense = Dense(num_classes, activation='softmax')\n","\n","  def forward_once(self, x):\n","        # Forward pass \n","        output = self.embedding(x)\n","        output = self.bilstm(output)\n","        return output\n","  \n","  def call(self, tweet, target, training):\n","    outputs = self.malstm_distance([self.forward_once(tweet),\n","                                   self.forward_once(target)])\n","    outputs = self.dense(outputs)\n","    return outputs\n","\n","def train_step(tweet_list, actual_v, target, sample_weights_list):\n","    tweet_list = tf.convert_to_tensor(tweet_list, dtype=tf.int32)\n","    target = tf.convert_to_tensor(target, dtype=tf.int32)\n","\n","    with tf.GradientTape(persistent=True) as tape:\n","        prediction = model(tf.convert_to_tensor(tweet_list), target)\n","        loss = cce(prediction, actual_v, sample_weight=sample_weights_list)\n","        epoch_accuracy.update_state(actual_v, prediction)\n","        \n","    grads = tape.gradient(loss, model.trainable_variables)\n","    grads = [grad if grad is not None else tf.zeros_like(var) for var, grad in zip(\n","        model.trainable_variables, grads)]\n","    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","    train_loss.update_state(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gNKyDY0wj7Ik","executionInfo":{"status":"ok","timestamp":1612372629922,"user_tz":-330,"elapsed":2377,"user":{"displayName":"Samiya Caur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2n0t4GHaw9xARSIfKBQgiUHxme9EkMwnzdS9usA=s64","userId":"02348889184279128207"}},"outputId":"638f5058-0cf5-469b-e419-d4e4652700ff"},"source":["model = Model(num_classes=num_class)\n","learning_rate = 2*1e-4\n","optimizer=keras.optimizers.Adam(learning_rate)\n","cce = tf.keras.losses.CategoricalCrossentropy()\n","epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n","train_loss = tf.keras.metrics.Mean(name='train_loss')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Qrt25KCNj7Io"},"source":["batches = []\n","\n","st = 0\n","while(st<X_train.shape[0]):\n","    if st+batch_size <X_train.shape[0]:\n","        batches.append([X_train[st:st+batch_size]['encoded_cleaned_tweet'],\n","                        y_train[st:st+batch_size],\n","                        X_train[st:st+batch_size]['target'],\n","                        sample_weights_train[st:st+batch_size]])\n","    else:\n","        batches.append([X_train[st:]['encoded_cleaned_tweet'],y_train[st:],\n","                        X_train[st:]['target'],\n","                        sample_weights_train[st:]])\n","\n","    st = st+batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wDA7AjKSj7Is","executionInfo":{"status":"ok","timestamp":1612373156399,"user_tz":-330,"elapsed":526341,"user":{"displayName":"Samiya Caur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2n0t4GHaw9xARSIfKBQgiUHxme9EkMwnzdS9usA=s64","userId":"02348889184279128207"}},"outputId":"6f7857e8-2e39-462e-92c4-d333a32b76a1"},"source":["pbar = tf.keras.utils.Progbar(target=n_epoch*len(batches), width=15, interval=0.005,\n","                              stateful_metrics=['train_loss', 'accuracy'])\n","\n","training_start_time = time()\n","for epoch in range(0,n_epoch):\n","    \n","    for encoded_tweet_list, tag, target, sample_weights_list in batches:\n","          tag_ = np.array(tag)\n","          tag_ = tag_.reshape(tag_.shape[0],1,tag_.shape[1])\n","          train_step(list(encoded_tweet_list), tag_, list(target), sample_weights_list)\n","          pbar.add(1, values=[(\"train_loss\", train_loss.result()),\n","                              (\"accuracy\", epoch_accuracy.result())])\n","    train_loss.reset_states()\n","    epoch_accuracy.reset_states()\n","\n","training_end_time = time()\n","print(\"Training time finished.\\n%d epochs in %12.2f\" % (n_epoch,\n","                                                        training_end_time - training_start_time))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["320/320 [===============] - 523s 2s/step - train_loss: 5.4687 - accuracy: 0.7340\n","Training time finished.\n","10 epochs in       523.45\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlCUXPlG3cgx","executionInfo":{"status":"ok","timestamp":1612373158148,"user_tz":-330,"elapsed":520486,"user":{"displayName":"Samiya Caur","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi2n0t4GHaw9xARSIfKBQgiUHxme9EkMwnzdS9usA=s64","userId":"02348889184279128207"}},"outputId":"906c2c07-3bb1-4db5-8dc8-2cbad2d0ec69"},"source":["Tar = [target_seq for _ in range(len(X_test))]\n","Tar = tf.convert_to_tensor(Tar)\n","pred_y = model(tf.convert_to_tensor(list(X_test['encoded_cleaned_tweet'])), Tar)\n","pred_y = tf.reshape(pred_y, [pred_y.shape[0], pred_y.shape[2]])\n","print(classification_report(np.argmax(y_test, axis=1), np.argmax(pred_y, axis=1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.84      0.89      0.86      1951\n","           1       0.55      0.18      0.27        60\n","           2       0.93      0.60      0.73       797\n","           3       0.04      0.10      0.06        58\n","           4       0.18      0.36      0.24       140\n","\n","    accuracy                           0.76      3006\n","   macro avg       0.51      0.43      0.43      3006\n","weighted avg       0.81      0.76      0.77      3006\n","\n"],"name":"stdout"}]}]}